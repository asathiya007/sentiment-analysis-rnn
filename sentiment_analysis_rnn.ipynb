{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This neural network performs sentiment analysis on text \n",
    "# Classifies text as positive or negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess the data\n",
    "import numpy as np\n",
    "from string import punctuation \n",
    "\n",
    "# fetch posts and labels\n",
    "with open('data/posts.txt', 'r') as f: \n",
    "    posts = f.read()\n",
    "with open('data/labels.txt', 'r') as f: \n",
    "    labels = f.read()\n",
    "    \n",
    "# standardize posts by lowering case and eliminating pronounciation \n",
    "posts = posts.lower()\n",
    "all_text = ''.join([c for c in posts if c not in punctuation])\n",
    "\n",
    "# split posts by new lines\n",
    "posts = all_text.split('\\n')\n",
    "\n",
    "# split labels by new lines \n",
    "labels = labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   ', 'story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly   ', 'homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home  the entertainment sets  a bathroom  pictures on the wall  a computer  and everything you once treasure to see what it  s like to be homeless  that is goddard bolt  s lesson   br    br   mel brooks  who directs  who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival  jeffery tambor  to see if he can live in the streets for thirty days without the luxuries if bolt succeeds  he can do what he wants with a future project of making more buildings  the bet  s on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can  t step off the sidewalk  he  s given the nickname pepto by a vagrant after it  s written on his forehead where bolt meets other characters including a woman by the name of molly  lesley ann warren  an ex  dancer who got divorce before losing her home  and her pals sailor  howard morris  and fumes  teddy wilson  who are already used to the streets  they  re survivors  bolt isn  t  he  s not used to reaching mutual agreements like he once did when being rich where it  s fight or flight  kill or be killed   br    br   while the love connection between molly and bolt wasn  t necessary to plot  i found  life stinks  to be one of mel brooks  observant films where prior to being a comedy  it shows a tender side compared to his slapstick work such as blazing saddles  young frankenstein  or spaceballs for the matter  to show what it  s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don  t know what to do with their money  maybe they should give it to the homeless instead of using it like monopoly money   br    br   or maybe this film will inspire you to help others   ']\n"
     ]
    }
   ],
   "source": [
    "# first 3 posts \n",
    "print(posts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n"
     ]
    }
   ],
   "source": [
    "# first 10 words \n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the words (map words to integers)\n",
    "from collections import Counter \n",
    "\n",
    "# create a list of words\n",
    "all_posts = ' '.join(posts)\n",
    "words = all_posts.split()\n",
    "\n",
    "# words mapped to frequency\n",
    "counts = Counter(words)\n",
    "\n",
    "# words sorted by frequency, in descending order\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# dict mapping each word to an integer, starting from 1 and increasing\n",
    "vocab_encoded = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# tokenize each post using the word-integer dict \n",
    "tokenized_posts = []\n",
    "for post in posts:\n",
    "    tokenized_posts.append([vocab_encoded[word] for word in post.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n"
     ]
    }
   ],
   "source": [
    "# first tokenized post \n",
    "print(tokenized_posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels: 1 is positive, 0 is negative \n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# first 3 encoded labels\n",
    "print(encoded_labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# obtain outlier (outliers determined by post length) stats \n",
    "post_lengths = Counter([len(post) for post in tokenized_posts])\n",
    "print('Zero length reviews: {}'.format(post_lengths[0]))\n",
    "print('Maximum review length: {}'.format(max(post_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove posts with length 0 (get posts with non-zero length) \n",
    "non_zero_idx = [ii for ii, post in enumerate(tokenized_posts) if len(post) != 0]\n",
    "tokenized_posts = [tokenized_posts[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts with non-zero length:  25000\n"
     ]
    }
   ],
   "source": [
    "# number of posts with non-zero length \n",
    "print('Number of posts with non-zero length: ', len(tokenized_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad or truncate all posts to a specific length (to pass to neural net)\n",
    "\n",
    "# pad/truncate all posts to they can be inputs to the neural network \n",
    "def mod_features(tokenized_posts, seq_length):\n",
    "    # getting the correct rows x cols shape \n",
    "    features = np.zeros((len(tokenized_posts), seq_length), dtype=int)\n",
    "    \n",
    "    # pad each post\n",
    "    for i, post in enumerate(tokenized_posts):\n",
    "        features[i, -len(post):] = np.array(post)[:seq_length]\n",
    "        \n",
    "    return features \n",
    "\n",
    "# all posts will be padded/truncated to 200 characters \n",
    "seq_length = 200\n",
    "\n",
    "# get padded/truncated posts\n",
    "features = mod_features(tokenized_posts, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  21025   308     6     3  1050   207     8  2138    32     1   171    57\n",
      "     15    49    81  5785    44   382   110   140    15  5194    60   154\n",
      "      9     1  4975  5852   475    71     5   260    12 21025   308    13\n",
      "   1978     6    74  2395     5   613    73     6  5194     1 24103     5\n",
      "   1983 10166     1  5786  1499    36    51    66   204   145    67  1199\n",
      "   5194 19869     1 37442     4     1   221   883    31  2988    71     4\n",
      "      1  5787    10   686     2    67  1499    54    10   216     1   383\n",
      "      9    62     3  1406  3686   783     5  3483   180     1   382    10\n",
      "   1212 13583    32   308     3   349   341  2913    10   143   127     5\n",
      "   7690    30     4   129  5194  1406  2326     5 21025   308    10   528\n",
      "     12   109  1448     4    60   543   102    12 21025   308     6   227\n",
      "   4146    48     3  2211    12     8   215    23]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0    63     4     3   125    36    47  7472  1395    16     3\n",
      "   4181   505    45    17     3   622   134    12     6     3  1279   457\n",
      "      4  1721   207     3 10624  7373   300     6   667    83    35  2116\n",
      "   1086  2989    34     1   898 46417     4     8    13  5096   464     8\n",
      "   2656  1721     1   221    57    17    58   794  1297   832   228     8\n",
      "     43    98   123  1469    59   147    38     1   963   142    29   667\n",
      "    123     1 13584   410    61    94  1774   306   755     5     3   819\n",
      "  10396    22     3  1724   635     8    13   128    73    21   233   102\n",
      "     17    49    50   617    34   682    85 28785 28786   682   374  3341\n",
      "  11398     2 16371  7946    51    29   108  3324]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35  1819    16\n",
      "    154    19   114     3  1305     5   336   147    22     1   857    12\n",
      "     70   281  1168   399    36   120   283    38   169     5   382   158\n",
      "     42  2269    16     1   541    90    78   102     4     1  3244    15\n",
      "     43     3   407  1068   136  8055    44   182   140    15  3043     1\n",
      "    320    22  4818 26224   346     5  3090  2092     1 18839 17939    42\n",
      "   8055    46    33   236    29   370     5   130    56    22     1  1928\n",
      "      7     7    19    48    46    21    70   344     3  2099     5   408\n",
      "     22     1  1928    16     3  3119   205     1 28787    21   281    68\n",
      "     38     3   339     1   700   715     3  3818  1229    22     1  1491\n",
      "      3  1197     2   283    21   281  2435     5    66    48     8    13\n",
      "     39     5    29  3244    12     6 21026 11723    13  2015     7     7\n",
      "   3687  2818    36  4147    36   374    15 11723   296     3   996   125\n",
      "     36    47   283     9     1   176   363  6893     5    94     3  2099\n",
      "     17     3  4976  2932 14557 19870     5    66    46    25    51   408\n",
      "      9     1  1928    16  3236   490   205     1 28787    46 11723  2845\n",
      "     25    51    80    48    25   483    17     3]]\n"
     ]
    }
   ],
   "source": [
    "# ensure one-to-one mapping between features and reviews\n",
    "assert len(features) == len(tokenized_posts), 'Features and reviews do not have one-to-one mapping.'\n",
    "\n",
    "# ensure that features are of the specified length \n",
    "assert len(features[0]) == seq_length, 'Features are not of specified length.'\n",
    "\n",
    "# print the first three modified posts \n",
    "print(features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into traning, validation, and testing data sets \n",
    "# x: features, y: labels\n",
    "\n",
    "# percentage of data set to be dedicated to training\n",
    "split_frac = 0.8\n",
    "\n",
    "# create traning data set \n",
    "split_idx = int(len(features) * split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "# create validation and testing data set by halves of leftover data\n",
    "split_idx = int(len(remaining_x) * 0.5)\n",
    "val_x, test_x = remaining_x[:split_idx], remaining_x[split_idx:]\n",
    "val_y, test_y = remaining_y[:split_idx], remaining_y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Training set: \t\t(20000, 200)\n",
      "Validation set: \t(2500, 200)\n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "# print out feature shapes of the data sets \n",
    "print('\\t\\t\\tFeature Shapes:')\n",
    "print('Training set: \\t\\t{}'.format(train_x.shape))\n",
    "print('Validation set: \\t{}'.format(val_x.shape))\n",
    "print('Test set: \\t\\t{}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders for the data \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor data sets \n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# set batch size\n",
    "batch_size = 50\n",
    "\n",
    "# create data loaders \n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,  304,   16,  457],\n",
      "        [  10,   14, 2802,  ...,   76,  479,   69],\n",
      "        [   0,    0,    0,  ..., 1629,    2,  659],\n",
      "        ...,\n",
      "        [1238,   22,    1,  ...,   10,   89,   23],\n",
      "        [   0,    0,    0,  ...,  568,    7,    7],\n",
      "        [ 530,    3,   20,  ...,  558,   32, 6953]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data \n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# train on GPU if available \n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu: \n",
    "    print('GPU available. Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available. Training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sentiment analysis recurrent neural network class\n",
    "import torch.nn as nn \n",
    "\n",
    "class SentimentAnalysisRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, \n",
    "                 n_layers, dropout_prob=0.5):\n",
    "        super(SentimentAnalysisRNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size \n",
    "        self.n_layers = n_layers \n",
    "        self.hidden_dim = hidden_dim \n",
    "        \n",
    "        # embedding and LSTM layers \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embeddings and lstm_out \n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid funciton \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        # return last sigmoid output and hidden state \n",
    "        return sig_out, hidden \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # initialize hidden state \n",
    "        weight = next(self.parameters()).data\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, \n",
    "                                 self.hidden_dim).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, \n",
    "                                self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, \n",
    "                                 self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, \n",
    "                                self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model with hyperparameters\n",
    "\n",
    "# + 1 for vocab size b/c of 0 padding & word tokens start at 1\n",
    "vocab_size = len(vocab_encoded) + 1 \n",
    "output_size = 1\n",
    "embedding_dim = 1000\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentAnalysisRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentAnalysisRNN(\n",
      "  (embedding): Embedding(74073, 1000)\n",
      "  (lstm): LSTM(1000, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print instance of sentiment analysis recurrent neural network \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions \n",
    "lr = 0.001 \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "Step: 100\n",
      "Loss: 0.642240\n",
      "Validation Loss: 0.648161\n",
      "Epoch: 1/1\n",
      "Step: 200\n",
      "Loss: 0.612374\n",
      "Validation Loss: 0.646742\n",
      "Epoch: 1/1\n",
      "Step: 300\n",
      "Loss: 0.586558\n",
      "Validation Loss: 0.578224\n",
      "Epoch: 1/1\n",
      "Step: 400\n",
      "Loss: 0.580955\n",
      "Validation Loss: 0.505853\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "\n",
    "# training param s\n",
    "epochs = 1 \n",
    "counter = 0 \n",
    "print_every = 100\n",
    "clip = 5\n",
    "\n",
    "# move model to GPU, if available \n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "# train for the set number of epochs \n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    # batch loop \n",
    "    for inputs, labels in train_loader: \n",
    "        counter += 1\n",
    "        \n",
    "        if train_on_gpu: \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "        # create new variables for hidden state to avoid backprop through\n",
    "        # entire training history \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        # zero accumulated gradients \n",
    "        net.zero_grad()\n",
    "        \n",
    "        # get the output from the model \n",
    "        output, h = net(inputs, h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # prevent the exploding gradient problem in RNNs and LSTMs\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # loss stats \n",
    "        if counter % print_every == 0:\n",
    "            # get validation loss \n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader: \n",
    "                \n",
    "                # create new variables for hidden state to avoid backprop\n",
    "                # through entire training history \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if (train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            net.train()\n",
    "            \n",
    "            print('Epoch: {}/{}'.format(e + 1, epochs))\n",
    "            print('Step: {}'.format(counter))\n",
    "            print('Loss: {:.6f}'.format(loss.item()))\n",
    "            print('Validation Loss: {:.6f}'.format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.513\n",
      "Test accuracy: 0.757\n"
     ]
    }
   ],
   "source": [
    "# get test data loss and accuracy \n",
    "\n",
    "test_losses = [] # for tracking loss \n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state \n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# evaluate test data \n",
    "for inputs, labels in test_loader: \n",
    "    \n",
    "    # create new variables for the hidden state to avoid backprop through\n",
    "    # the entire training history \n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    if train_on_gpu: \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "    # get predicted outputs \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss \n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to prediction (0 or 1)\n",
    "    # by rounding to the nearest integer \n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    # compare prediction to true label \n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    if train_on_gpu:\n",
    "        correct = np.squeeze(correct_tensor.numpy())\n",
    "    else: \n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "# print stats \n",
    "print('Test loss: {:.3f}'.format(np.mean(test_losses)))\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print('Test accuracy: {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on a post \n",
    "from string import punctuation \n",
    "\n",
    "def tokenize_post(post):\n",
    "    # make text lowercase, eliminate punctuation, and split by spaces\n",
    "    post = post.lower()\n",
    "    text = ''.join([c for c in post if c not in punctuation])\n",
    "    words = text.split()\n",
    "    \n",
    "    # tokens \n",
    "    tokenized_post = []\n",
    "    tokenized_post.append([vocab_encoded[word] for word in words])\n",
    "    \n",
    "    return tokenized_post\n",
    "\n",
    "def mod_features(tokenized_posts, seq_length):\n",
    "    # getting the correct rows x cols shape \n",
    "    features = np.zeros((len(tokenized_posts), seq_length), dtype=int)\n",
    "    \n",
    "    # pad each post\n",
    "    for i, post in enumerate(tokenized_posts):\n",
    "        features[i, -len(post):] = np.array(post)[:seq_length]\n",
    "        \n",
    "    return features\n",
    "\n",
    "def predict(net, post, seq_length=200):\n",
    "    \n",
    "    net.eval() \n",
    "    \n",
    "    # tokenize post \n",
    "    tokenized_post = tokenize_post(post)\n",
    "    \n",
    "    # pad/truncate tokenized post \n",
    "    features = mod_features(tokenized_post, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into model \n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # init hidden state \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if train_on_gpu: \n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "        \n",
    "    # get the output from the model \n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to prediction (0 or 1)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    # describe result \n",
    "    if (pred.item() == 1):\n",
    "        print('Positive post detected!')\n",
    "    else:\n",
    "        print('Negative post detected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postive and negative posts \n",
    "post_neg = 'Today was easily the worst day of my life. Everything was so disorganized and nothing got done!'\n",
    "post_pos = 'Today was without a doubt the best day of my life. I was so energized and productive!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative post detected!\n"
     ]
    }
   ],
   "source": [
    "# make a prediction on the negative post \n",
    "predict(net, post_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive post detected!\n"
     ]
    }
   ],
   "source": [
    "# make a prediction on the positive post \n",
    "predict(net, post_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
